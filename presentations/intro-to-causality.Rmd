---
title: "Wet-Blanketing 101"
subtitle: "Intro to Causal Inference"
author: "Dan Ovando"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(echo = FALSE, dev = "svg", message = FALSE, warning = FALSE,
                      fig.align = "center")
library(tidyverse)
library(tidymodels)
library(modelsummary)
library(gt)
library(hrbrthemes)
pres_theme <- theme_ipsum(base_size = 16, axis_title_size = 16)

theme_set(pres_theme)
set.seed(42)
```

class: center, middle, inverse
# Releasing your Inner Skeptic
## (Very) Intro to Causal Inference
### Dan Ovando - Hilborn/Fay Lab Meeting
### `r Sys.Date()`


```{r}
knitr::include_graphics("https://media.giphy.com/media/KtbPylMXJ6IPS/giphy.gif")
```


---


# Is Wine Good For You?

.pull-left[

Seems like it should be an easy question, but we get a new answer every slow news day

Why?

- Can't randomly assign wine use

- More money == more wine?

- Depends on *how much* wine

- Long-term effects

- etc. 
] .pull-right[

<br>
<br>
```{r}
knitr::include_graphics("https://media.giphy.com/media/E3L5goMMSoAAo/giphy.gif")
```


]

---

# Do Management Interventions Work?

Answering "is wine good for you" seems like it should be easier than say what makes an ITQ work...


So what does this say about program evaluation in the natural world?
  
--

<br>
<br>
<br>
.center[***Finding a clear-cut causal effect should be pretty rare***]


---




# Less DrumLines = More Sharks?

<br>
<br>

> Banning drumlines (29% (−13–52%)) or moving towards more selective hook and line fishing (25% (−8–48%)) were estimated to be less effective but may be more-acceptable management interventions in some contexts -- MacNeil et al. 2020


Our goals for today: 
  
  - What do we need to assume to make this statement true?
  
  - Are those assumptions reasonable for this paper?


---

# 
```{r, fig.align="center"}
knitr::include_graphics(here::here("imgs", "inference.jpeg"))
```


---

# What Is Causal Inference?

The Rubin Causal Model says that the causal effect of a treatment $T$ on outcome $y$ for unit $i$ is

$$y_i(T = 1) - y_i(T = 0)$$
  
Seems straightforward enough? What's the problem here?

--

.center[The fundamental problem of causal inference: *We can't observe the exact same unit both with and without treatment*]
 
```{r out.width = '40%', echo = FALSE, fig.align = "center"}
knitr::include_graphics("https://media.giphy.com/media/DUyLrL6MIFBsI/giphy.gif") 
```
 

---

# What Is Causal Inference?

Since we can't observe the same unit with and without treatment, we have to turn to statistics:


Estimate the mean difference between treated and untreated groups

$$E[y|T=1] - E[y|T=0]$$

In order for this to pass the "causal" test, ***treatment has to be as good as randomly assigned***

$$y_i = \beta_{T=0} +\beta_{t=1}T_i + \epsilon_i$$

$$Cov(T_i,\epsilon_i) = 0$$


---

# What is Causal Inference?

This is why randomized control trials are so powerful: Easy to argue that any differences between control and treatment groups are because of the treatment

But, RCTs are hard / bordering impossible in natural resource management


Ask yourself:

.center[*How close to a randomized control trial is the paper I'm reading?*]



---

# Why Does This Matter?

.pull-left[

we can never **proove** causality
  - Easy to always be the "but what about causality" person
  - Give me good descriptive stats over bad causal inference any day

But, causality really can matter
  - Omitted Variable Bias
  - Collider Bias
  - Reverse Causality

Are all **really** common in natural settings and can cause **big** problems

] 
.pull-right[

```{r out.width = '100%', echo = FALSE, fig.align = "center"}
knitr::include_graphics("https://imgs.xkcd.com/comics/correlation.png") 
```

]

---

# Omitted Variable Bias


As the name implies, leaving out an important variable can bias your estimate of the causal effect of your treatment 

.center[*What defines "important"*]

--


Under what conditions is 

$$y_i = \beta_0 +\beta_1T_i + \beta_2w_i + \epsilon_i$$

Equivalent to

$$y_i = \beta_0 +\beta_1T_i + \epsilon_i$$

---

# Omitted Variable Bias

$$y_i = \beta_0 +\beta_1T_i + \beta_2w_i + \epsilon_i$$
 
In this model, you can show that if you leave $w$ out of your regression, your estimate of $\beta_1$, the thing you care about, will be biased according to

$$\lim_{N\to\infty} \hat{\beta_1} = \beta_1 + \color{red}{\beta_2}\color{blue}{cor(T,w)}\frac{\sigma_w}{\sigma_T}$$
OVB is a function of

The correlation between the omitted variable and the outcome $\color{red}{\beta_2}$
 
The correlation between the treatment and the omitted variable $\color{blue}{cor(T,w)}$

---


# Omitted Variable Bias in Action

True Model: MPAs are independent of penguins $P$, penguins affect fish biomass

$$y_i = \beta_0 + \beta_1MPA_i + \beta_2P_i + \epsilon_i$$


$$\beta_{penguin} = -0.2$$

$$cor(MPA,P) = 0$$

Estimated Model

$$y_i = \beta_0 + \beta_1MPA_i + \epsilon_i$$


```{r, echo = FALSE}


b0 <-  0

b1 <-  .2

b2 <-  -.2

sigma <-  0.05

samps <- 1000

dat <- tibble(mpa = sample(c(0, 1), samps, replace = TRUE),
              penguins = rnorm(samps)) %>%
  mutate(fish = rnorm(samps, b0 + b1 * mpa + b2 * penguins, sigma))

```

---


# Omitted Variable Bias in Action

.pull-left[
True Model: 

MPAs are independent of penguins $P$, penguins affect fish biomass

$$y_i = \beta_0 + \beta_1MPA_i + \beta_{penguin}P_i + \epsilon_i$$

$$\beta_{penguin} = -0.2$$

$$cor(MPA,P) = 0$$

Estimated Model

$$y_i = \beta_0 + \beta_1MPA_i + \epsilon_i$$

MPA Effect = $\color{red}{\text{`r b1`}}$

```{r, echo = FALSE}
mod1 <- lm(fish ~ mpa, data = dat)
```
] .pull-right[
```{r, echo=FALSE}
msummary(list("Penguins ind. of MPA" = mod1), "gt") %>% 
       tab_style(style = cell_text(color = 'red'),
              locations = cells_body(rows = 3))
```
]
---

#  Omitted Variable Bias in Action
.pull-left[
True Model: 

MPAs are correlated with penguins $P$, penguins don't affect fish biomass

$$y_i = \beta_0 + \beta_1MPA_i + \beta_{penguin}P_i + \epsilon_i$$


$$\beta_{penguin} = 0$$

$$cor(MPA,P) > 0$$

Estimated Model

$$y_i = \beta_0 + \beta_1MPA_i + \epsilon_i$$

MPA Effect = $\color{red}{\text{`r b1`}}$
] .pull-right[
```{r, echo = FALSE}
b0 <-  0

b1 <- .2

b2 <-  0

cor <- 0.5

sigma <-  0.05

samps <- 1000

dat <- tibble(mpa = sample(c(0, 1), samps, replace = TRUE),
              penguins = rnorm(samps) + mpa * cor) %>%
  mutate(fish = rnorm(samps, b0 + b1 * mpa + b2 * penguins, sigma))

mod2 <- lm(fish ~ mpa, data = dat)


```

```{r, echo = FALSE}
msummary(models = list("Penguins ind. of MPA" = mod1, "Penguins ind. of fish" = mod2), "gt") %>% 
       tab_style(style = cell_text(color = 'red'),
              locations = cells_body(rows = 3))

```

]


---

# Omitted Variable Bias in Action

True Model: 

MPAs are correlated with penguins $P$, penguins affect fish biomass

$$y_i = \beta_0 + \beta_1MPA_i + \beta_{penguin}P_i + \epsilon_i$$


$$\beta_{penguin} = 0.2$$

$$cor(MPA,P) > 0$$

Estimated Model

$$y_i = \beta_0 + \beta_1MPA_i + \epsilon_i$$

MPA Effect = $\color{red}{\text{`r b1`}}$

---

# Omitted Variable Bias in Action
MPA Effect = $\color{red}{\text{`r b1`}}$

```{r, echo = FALSE}

b0 <-  0

b1 <- .2

b2 <-  -0.2

cor <- 0.5

sigma <-  0.05

samps <- 1000

dat <- tibble(mpa = sample(c(0, 1), samps, replace = TRUE),
              penguins = rnorm(samps) + mpa * cor) %>%
  mutate(fish = rnorm(samps, b0 + b1 * mpa + b2 * penguins, sigma))

mod3 <- lm(fish ~ mpa, data = dat)

msummary(models = list("Penguins ind. of MPA" = mod1, "Penguins ind. of fish" = mod2,
                       "Penguins = MPA & Penguins eat Fish" = mod3),output = "gt") %>% 
     tab_style(style = cell_text(color = 'red'),
              locations = cells_body(rows = 3)) %>% 
   tab_options(
    column_labels.font.size = "smaller",
    table.font.size = "smaller",
    data_row.padding = px(3)
  )



```


---


# Omitted Variable Bias In Action

.pull-left[
What's the solution to OVB?

Put a bird (coefficient) on it!

Include all the variables
  - If they would have caused OVB, problem solved
  - If not, who cares (prediction cares, but we don't care about that)
  - This is why stepwise AIC selection is a **bad idea** if you care about causality
  

Identification strategy known as "selection on observables"

.center[*I am able to observe and control for every possible confounding variable, so my treatment effect is causal*]

] .pull-right[

```{r}
knitr::include_graphics("https://64.media.tumblr.com/tumblr_lps76hjF2M1qzu1tpo1_400.gif")
```


]


---

# Collider Bias - Nothing is ever easy

I want to study the effect of MPA size on fish biomass

Someone (Ray) says that I need to control for the effect of fishing pressure

MPAs affect fishing pressure, fishing pressure affects fish, seems like classic OVB

So, I control for both MPA size and fishing pressure, problem solved!


---

# Collider Bias - Nothing is ever easy <sup>1</sup>

.pull-left[
Suppose there's another factor out there though: *strengh of management*

Higher management strength causes lower fishing mortality, but also affects fish biomass through other pathways (e.g. restrictions on polluting activities near our site)

Fishing pressure is now a "collider": what happens if we include it in our regression?

] .pull-right[


```{r}
knitr::include_graphics(here::here("imgs","collider.jpeg"))
```


]
.footnote[
[1] Inspired by Statistical Rethinking 2nd Edition, by Richard McElreath
]

---

# Collider Bias - Nothing is ever easy
.left-column[

```{r}
intercept <-  0

beta_mpa <- 0

beta_f <-  -0.2

b_management <- -1

beta_mpa_f <- -0.5

sigma <-  0.05

samps <- 2000

mpa = sample(c(0, 1), samps, replace = TRUE)

mpa = rnorm(samps)

management = rnorm(samps)

f <- rnorm(samps) + mpa * beta_mpa_f + b_management * management

fish <- rnorm(samps, intercept + beta_mpa * mpa + beta_f * f + -b_management * management)

dat <- tibble(mpa, management, f, fish)

# dat %>% 
#   ggplot(aes(mpa, fish)) + 
#   geom_point()
```


True effect of fishing: `r beta_f`

True direct effect of MPA: `r beta_mpa`

True direct effect management: `r b_management`

] .right-column[



```{r}


mod1 <- lm(fish ~ mpa + f - 1, data = dat)

mod2 <- lm(fish ~ mpa + f + management - 1, data = dat)

msummary(list(
  "Collider Bias" = mod1,
  "Control for Management" = mod2
),
output = "gt") %>%
  tab_style(style = cell_text(color = 'red'),
            locations = cells_body(rows = 3))
```
]
---

# Collider Bias 

Fishing mortality is correlated with MPA and has an affect on fish - I thought you said controlling for it would save me from omitted variable bias!!


```{r, echo = FALSE, fig.width=8, fig.height=4}

dat %>% 
  mutate(frank = percent_rank(f)) %>% 
  filter(between(frank, 0.45, 0.65)) %>% 
  ggplot(aes(mpa, fish, fill = management)) + 
  geom_hline(aes(yintercept = 0)) +
  geom_point(shape = 21, alpha = 0.75, size = 4) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_fill_gradient2(low = "tomato", high = "steelblue", mid = "white", midpoint = 0,guide = guide_colorbar(frame.colour = "black", barheight = unit(10, "lines"), nbin = 1000)) + 
  labs(title = "For Observations with Average Fishing Pressure")

```

**Knowing fishing mortality and knowing MPA implicitly tells us something about management, and management affects fish**


???

---

# Reverse Causality
It's bad....
.pull-left[
```{r}

t <-  100

f_on_p = -0.02

p_on_f = -0.05

fish <- rep(0, 100)

price <-  rep(1, 100)

for (i in 2:t){
  
  price[i] = rnorm(1,price[i - 1] + f_on_p * fish[i - 1],1)
  
  
  fish[i] = rnorm(1,1.01 * fish[i - 1] + p_on_f * price[i],1)

}

dat <- tibble(price, fish, t = 1:t)


dat %>% 
  pivot_longer(c(-t), names_to = "variable", values_to = "value") %>% 
  ggplot(aes(t, value, color = variable)) + 
  geom_line(size = 2) + 
 theme(legend.position = "top") +
  scale_color_discrete(name = '')
  


```
] .pull-right[

True effect of price on fish = $\color{red}{\text{`r p_on_f`}}$

```{r}
msummary(models = list("Fish on Price" = lm(fish ~ price, data = dat)), output = "gt") %>% 
    tab_style(style = cell_text(color = 'red'),
            locations = cells_body(rows = 3))
```

]

---

# Remove your Hidden Penguins!

.pull-left[
Selection on observables can work!

But, there's always a risk that there's something you didn't measure out there messing you up

[Identification strategies](https://scunning.com/causalinference_norap.pdf) help control for unobserved confounders with fewer assumptions than "selection on observables"

- Difference-in-difference
  - Synthetic Controls
- Propensity Scores
- Within-Estimator
- Regression Discontinuity 
- Instrumental Variables (reverse causality)
- [Empirical Dynamic modeling](https://science.sciencemag.org/content/338/6106/496) & [DAGs](http://bayes.cs.ucla.edu/jp_home.html)

] .pull-right[

```{r}
knitr::include_graphics("https://media.giphy.com/media/Wz7gk4e2Pxcmk/giphy.gif")
```


]
---


class: center, middle

# How close is this study to a randomized experiment?

# How does it try and control for unobserved confounders?

---

class: center, middle, inverse

# Evaluating Causal Claims

---


# No Causation Without Manipulaion

In other words, if you want to know the effect of something, you need to observe what happens when that thing changes

.center[*Where is the "experiment" in the study?*]

- Before-after?
  - What else might have changed?

- With-Without?
  - Is space a reasonable substitute for time?

- Synthetic Control?
  - Is it a good out-of-sample predictor?
  
  **Reverse Causality**

---

# Is there a clear identification strategy?

This is something that econ does very well: hard to even publish on empirical program evaluation without a clear identification strategy. 

- Is there a mechanisms for controlling for unobserved confounders?
  
  - e.g. difference-in-difference / BACI
  
  - Is the strategy valid? Are spatial controls outside an MPA unaffected by the treatment?

- If identification strategy is basically selection on observables...
  
  - Is this acknowledged and evaluated in the text?
    - How do they think treatment is assigned and why do the variables they include deal with it? Why is risk of OVB minimal?
  
  - Selection on observables + any kind of AIC style model selection == **danger**
    - AIC tells you about *prediction*, not *causality*

---

# Do the results apply outside the study?

Suppose I run a study on the effects of motivational halftime speeches on NBA scoring performance, and find that they increase shot accuracy by 10%. 

Does that mean that if you give me a motivational speech then put me in the second half of a playoff basketball game that I'll be 10% more accurate? **probably not**

This is the difference in **average treatment effect on the treated (ATT)** and **average treatment effect**. 

If a study claims to be estimating average treatment effects, how reasonable is it the the same model used to estimate the causal effect would apply to the broader population?


---

# In Defense of R<sup>2</sup>

R<sup>2</sup> measures the proportion of the variance in the outcome explained by the model. 

It is entirely possible to have a model that estimates a precise and meaningful causal effect, but has a very low R<sup>2</sup>

.center[*but what does this imply*?]

--

- Most of the variance is explained by something other than the treatment
  - Good to know if a policy drives the bus or is hanging off the back bumper
  - There is a lot of potential for OVB etc.
    - Have to assume either all other variance is noise, or independent of included data!

- This implies that we have a very powerful model


---

# "Significance" and friends
.pull-left[

**Frequentist**
- If you're playing by frequentist rules, you have to play by the rules
  - Confidence intervals are not credible intervals
- If p = 0.0499999, I'd like to see results for other model configurations

**Bayesian**
- No such thing as "significance"
- No more arbitrary thresholds! Yay. 
- But where do you draw the line?
  - Does text reflect the actual posterior probability distributions?

].pull-right[
<br>
<br>
```{r}
knitr::include_graphics(here::here("imgs", "creds.png"))

```


]

???
Our civil society metric (voice and accountabil- ity) was associated with a higher likelihood of sharks being observed. In addition, nations with larger coastal populations coincided with sharks not being observed, whereas we found little evidence for an effect of increased national wealth (through the human development index).

Estimated relative effect sizes for the influence of national socio-economic conditions (including the human development index (HDI)) on the expected proportion of negative binomial variates on BRUVS sets
. The reported values are the highest posterior density median values (circles), with 50% (thick lines) and 90% (thin lines) uncertainty intervals. Black symbols indicate that the 90% uncertainty intervals did not overlap zero; grey symbols indicate that the 50% uncertainty intervals did not overlap zero; and white symbols indicate that the 50% uncertainty intervals did overlap zero.
---

# Effect Sizes

- Big red flag if p-values are reported without effect sizes!

- For policy evaluation, there's a big difference between MPAs having a significant 0.01% effect size and a significant 50% effect size
  - Show me the intervals!
  
- Small and significant effect implies really powerful model

- Large and significant effect
  - Cool! but.....
  - Big and clear effects in complex systems are probably rare
  - Is this a well-studied system: why has no-one else found it before?
  - Is it too good to be true?

---

# Effect Sizes

.pull-left[

```{r, out.width='150%'}
knitr::include_graphics(here::here("imgs","when-contact.png"))
```


]
.pull-right[
> A difference of 0.8 on a five-point scale . . . wow! You rarely see this sort of thing...So we’re talking about a causal effect that’s a full 40% of what is pretty much the maximum change imaginable. Wow, indeed. And, judging by the small standard errors (again, see the graphs above), these effects are real, not obtained by capitalizing on chance or the statistical significance filter or anything like that. -- [Andrew Gelman](https://www.washingtonpost.com/news/monkey-cage/wp/2015/05/20/fake-study-on-changing-attitudes-sometimes-a-claim-that-is-too-good-to-be-true-isnt/?arc404=true)

]


---

# Just-So Stories

.pull-left[

- "The results that fit our narrative mean that our hypotheses were right"

- "The results that don't fit our narrative mean that there was a problem with those data"

- Possible, but is it likely?


] .pull-right[
```{r,out.width = '75%'}
knitr::include_graphics("https://offtheshelf.com/wp-content/uploads/2014/08/justsostories.jpg")
```

]


---


# Last and (I think) Least - Technical model details!

This may be controversial. IMO we focus a LOT on questions like log-normal vs gamma, random vs. fixed effects, X's amazing test of awesomeness vs Y's even better test of greatness

In my experience, these things matter, but often at the margin
  
  - Matter if  you're hunting for significance....
  
  - They often don't change the *story*

.center[**Causal inference problems get less attention but can mess you up a lot more than not using the exact right statistical test**]

  


---

# Back to the drumlines.....

> Banning drumlines (29% (−13–52%)) or moving towards more selective hook and line fishing (25% (−8–48%)) were estimated to be less effective but may be more-acceptable management interventions in some contexts -- MacNeil et al. 2020

.center[What do we think?]

???

I think this is a good example because it's not clear cut right or wrong

Thanks Charmane, lots of interesting stuff in here. Might be worth a dedicated lab discussion some time in the future as a case study in challenges of talking about statistical results, particularly in a short-format journal written with appeal to a broad audience in mind. If we were playing by "traditional" frequentist rules, roughly speaking few of the variables in Figure 1 would be "significant" at the 0.05 level, or mostly even at the 0.1 level. I'm not suggesting that we should play by those rules, but it's interesting to think that were this same study to be published say 10 years ago in a frequentist manner, technically speaking many of the variables that they talk about in the paper aren't "significant". I.e. in a frequentist sense, we would probably say "Our civil society metric (voice and accountability) was not significantly associated with a higher likelihood of sharks being observed", as opposed to "Our civil society metric (voice and accountability) was associated with a higher likelihood of sharks being observed". Which, technically speaking, there is greater probability of a positive association between civil society metric and sharks, but also a greater than 10% probability that the associate is negative. Makes you wonder how our interpretation of a lot of old frequentist studies would change if we viewed them in a Bayesian light.

There's some confusing interpretation of variables. For example, they state that

"Banning drumlines (29% (−13–52%)) or moving towards more selective hook and line fishing (25% (−8–48%)) were estimated to be less effective but may be more-acceptable management interventions in some contexts"

But, according to figure two, drumlines asnd hook and line have roughly opposite mean effect sizes (and I'm not clear from those figures whether presence for things like a fishing fear refers to presence of the gear or presence of management of that gear?). For either interpretation, both hook and line and drumline can't mean the same thing: One of them, according to their results, has more posterior support for a negative effect on shark MaxN. Unless they're doing something like "presence" means presence of drumlines for drumlines, but means rule saying you have to only use hook and line for hook and line?. I'm all for moving beyond p-value of bust, but it does make it hard to talk about these kinds of results when nontrivial parts of the posterior effect size cross zero. Even if the mean is for a positive effect size, if the posterior crosses zero meaningfully there's also evidence for a negative effect that can't just be ignored. Referring to variables with support for a negative effect (both  banning drumlines or moving towards more selective hook size have) as  "less effective" is a little off IMO; most readers would take that as saying that they work, just less than something else, when according to their results there is  support for a positive or a negative effect.

Some causal interpretation fun as well! This study doesn't techincally answer the question about banning drumlines or moving towards selective hooks. It estimates the posterior probability density of maxN for places that do/do not have drumlines / hook and line. It could be that people use drumlines in places that for exogenous reasons have less sharks and so sharks are harder to find (maybe overfishing of prey species by other gears). If that was the case, banning druminlines wouldn't increase sharks. Not saying that's the case, I'd agree that it's more plausible that having a lot of drumlines around is tough on sharks (exhibit A, dusky shark caught on a drumline ,safely tagged and released though!), but worth thinking about what exactly the stastical analysis does and does not tell us.

Anyway, happy friday from the lab's resident wet blanket!

Dan


---


# 
```{r, fig.align="center"}
knitr::include_graphics(here::here("imgs", "drumlines.jpeg"))
```


---

class: center, middle, inverse

# extras

---

